{"name":"Practical-machine-learning","tagline":"Practical Machine Learning Course Project","body":"# Practical Machine Learning - Course Project\r\nSai krishnan  \r\nSaturday, February 21, 2015  \r\n##Background  \r\nUsing devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. In this project, data from accelerometers on the belt, forearm, arm, and dumbell of the 6 participants was collected. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. Our goal is to build a model based on sensor data to predict whether the lift was performed correctly or not. \r\n\r\nData Sources  \r\n\r\nTraining dataset: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  \r\nTest dataset: https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv  \r\n\r\nOutcome variable: \"classe\"  \r\n- lift exactly according to the specification (category 'A')  \r\n- throwing the elbows to the front            (category 'B')  \r\n- lifting the dumbbell only halfway           (category 'C')  \r\n- lowering the dumbbell only halfway          (category 'D')  \r\n- throwing the hips to the front              (category 'E')  \r\n\r\n## Data Cleaning & Exploratory Analysis  \r\n  \r\n\r\n```r\r\n# Load the training and test datasets from working directory; replace missing values with 'NA'\r\nharTrain <- read.csv(\"C:/Users/Admin/Documents/Practical Machine Learning/Project/pml-training.csv\",header=TRUE,na.strings=c(\"NA\",\"\")) \r\nharTest <- read.csv(\"C:/Users/Admin/Documents/Practical Machine Learning/Project/pml-testing.csv\",header=TRUE,na.strings=c(\"NA\",\"\"))\r\n\r\n# Delete columns with missing values\r\nharTrain <- harTrain[,colSums(is.na(harTrain)) == 0]\r\nharTest  <-  harTest[,colSums(is.na(harTest))  == 0]\r\n\r\n# Since we want to predict the type of lift using only activity monitor data, \r\n# the other variables (in columns 1 though 7) are eliminated from the datasets  \r\nharTrain <- harTrain[,-c(1:7)]\r\nharTest  <- harTest [,-c(1:7)]\r\n\r\n# Compare the sizes of the training and testing datasets  \r\ndim(harTrain); dim(harTest)\r\n```\r\n\r\n```\r\n## [1] 19622    53\r\n```\r\n\r\n```\r\n## [1] 20 53\r\n```\r\n\r\n```r\r\n# Summarize the values in the outcome variable (classe) of the training dataset\r\nsummary(harTrain$classe)\r\n```\r\n\r\n```\r\n##    A    B    C    D    E \r\n## 5580 3797 3422 3216 3607\r\n```\r\nWe see that instances of correct lift (classe = 'A') are most (at 5580) as compared to the occurrences of each of the incorrect lifts.  \r\n\r\n## Data Preparation  \r\nThe training dataset is fairly large with 19622 rows and 53 columns. We partition the training dataset using random sampling without replacement into the following 2 datasets to allow cross validation:  \r\n- harTrng (80% of training data)    \r\n- harVal  (20% of training data)  \r\n\r\n\r\n```r\r\nlibrary(caret)\r\n```\r\n\r\n```\r\n## Warning: package 'caret' was built under R version 3.1.2\r\n```\r\n\r\n```\r\n## Loading required package: lattice\r\n## Loading required package: ggplot2\r\n```\r\n\r\n```r\r\nset.seed(12345) # For reproducibility\r\ninTrain <- createDataPartition(y=harTrain$classe, p=0.80, list=FALSE)\r\nharTrng <- harTrain[inTrain,]\r\nharVal  <- harTrain[-inTrain,]\r\n```\r\nThe core training dataset contains 15699 rows and the validation dataset contains 3923 rows.  Comparing the the values of the training an validation sets:- \r\n\r\n```r\r\npar(mfrow=c(1,2))\r\nplot(harTrng$classe, col=\"blue\", xlab=\"classe levels in Training dataset\", ylab=\"Frequency\")\r\nplot(harVal$classe, col=\"blue\", xlab=\"classe levels in validation dataset\", ylab=\"Frequency\")\r\n```\r\n\r\n![](./PML-Project_files/figure-html/unnamed-chunk-3-1.png) \r\n\r\n```r\r\n#Write to PNG file\r\ndev.copy(png, file = \"Plot1.png\")\r\n```\r\n\r\n```\r\n## png \r\n##   3\r\n```\r\n\r\n```r\r\ndev.off()\r\n```\r\n\r\n```\r\n## pdf \r\n##   2\r\n```\r\nWe observe that the distribution of outcomes is similar in both the training and validation datasets.  \r\n\r\n##Model Development for predicting exercise correctness  \r\n\r\n```r\r\n#Load required packages\r\nlibrary(MASS)\r\nlibrary(rpart)\r\nlibrary(rpart.plot)\r\n```\r\n\r\n```\r\n## Warning: package 'rpart.plot' was built under R version 3.1.2\r\n```\r\n\r\n```r\r\nlibrary(rattle)\r\n```\r\n\r\n```\r\n## Warning: package 'rattle' was built under R version 3.1.2\r\n```\r\n\r\n```\r\n## Rattle: A free graphical interface for data mining with R.\r\n## Version 3.4.1 Copyright (c) 2006-2014 Togaware Pty Ltd.\r\n## Type 'rattle()' to shake, rattle, and roll your data.\r\n```\r\nUsing Decision Trees algorithm  \r\n\r\n```r\r\ndtModel      <- rpart(classe ~ ., data=harTrng, method=\"class\")\r\ndtPrediction <- predict(dtModel, harVal, type = \"class\")\r\nfancyRpartPlot(dtModel)\r\n```\r\n\r\n```\r\n## Warning: labs do not fit even at cex 0.15, there may be some overplotting\r\n```\r\n\r\n![](./PML-Project_files/figure-html/unnamed-chunk-5-1.png) \r\n\r\n```r\r\n#Write to PNG file\r\ndev.copy(png, file = \"Plot2.png\")\r\n```\r\n\r\n```\r\n## png \r\n##   3\r\n```\r\n\r\n```r\r\ndev.off()\r\n```\r\n\r\n```\r\n## pdf \r\n##   2\r\n```\r\n\r\n```r\r\n#Plot confusion matrix\r\nconfusionMatrix(dtPrediction, harVal$classe)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1003  165    9   77   12\r\n##          B   21  390   72   23   50\r\n##          C   36   64  542  101   92\r\n##          D   33   58   45  408   36\r\n##          E   23   82   16   34  531\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.7326          \r\n##                  95% CI : (0.7185, 0.7464)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.6604          \r\n##  Mcnemar's Test P-Value : < 2.2e-16       \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.8987  0.51383   0.7924   0.6345   0.7365\r\n## Specificity            0.9063  0.94753   0.9095   0.9476   0.9516\r\n## Pos Pred Value         0.7923  0.70144   0.6491   0.7034   0.7741\r\n## Neg Pred Value         0.9575  0.89041   0.9540   0.9297   0.9413\r\n## Prevalence             0.2845  0.19347   0.1744   0.1639   0.1838\r\n## Detection Rate         0.2557  0.09941   0.1382   0.1040   0.1354\r\n## Detection Prevalence   0.3227  0.14173   0.2128   0.1478   0.1749\r\n## Balanced Accuracy      0.9025  0.73068   0.8510   0.7910   0.8440\r\n```\r\nWe observe that the accuracy of the decision trees algorithm is 73.26% with an out of sample error at 26.74%.  \r\n\r\nUsing Naive Bayes algorithm  \r\n\r\n```r\r\nlibrary(e1071) #Package required to run Naive Bayes algorithm\r\n```\r\n\r\n```\r\n## Warning: package 'e1071' was built under R version 3.1.2\r\n```\r\n\r\n```r\r\nnbModel      <- naiveBayes(classe ~ ., data=harTrng)\r\nnbPrediction <- predict(nbModel, newdata=harVal)\r\n#Plot confusion matrix\r\nconfusionMatrix(nbPrediction, harVal$classe)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction   A   B   C   D   E\r\n##          A 327  40  15   0  11\r\n##          B 103 456  57  27 155\r\n##          C 512 159 494 235  92\r\n##          D 141  49  86 307 108\r\n##          E  33  55  32  74 355\r\n## \r\n## Overall Statistics\r\n##                                         \r\n##                Accuracy : 0.4943        \r\n##                  95% CI : (0.4785, 0.51)\r\n##     No Information Rate : 0.2845        \r\n##     P-Value [Acc > NIR] : < 2.2e-16     \r\n##                                         \r\n##                   Kappa : 0.3766        \r\n##  Mcnemar's Test P-Value : < 2.2e-16     \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity           0.29301   0.6008   0.7222  0.47745  0.49237\r\n## Specificity           0.97649   0.8919   0.6919  0.88293  0.93941\r\n## Pos Pred Value        0.83206   0.5714   0.3311  0.44428  0.64663\r\n## Neg Pred Value        0.77649   0.9030   0.9218  0.89604  0.89152\r\n## Prevalence            0.28448   0.1935   0.1744  0.16391  0.18379\r\n## Detection Rate        0.08335   0.1162   0.1259  0.07826  0.09049\r\n## Detection Prevalence  0.10018   0.2034   0.3803  0.17614  0.13994\r\n## Balanced Accuracy     0.63475   0.7463   0.7071  0.68019  0.71589\r\n```\r\nThe model accuracy drops further to nearly 50% using the Naive Bayes algorithm. The out of sample error is very high in this case at nearly 50%.  \r\n\r\nUsing Random Forest algorithm  \r\n\r\n```r\r\nlibrary(randomForest)  #Package required to run random forest algorithm\r\n```\r\n\r\n```\r\n## Warning: package 'randomForest' was built under R version 3.1.2\r\n```\r\n\r\n```\r\n## randomForest 4.6-10\r\n## Type rfNews() to see new features/changes/bug fixes.\r\n```\r\n\r\n```r\r\nset.seed(12345)\r\nrfModel      <- randomForest(classe ~. , data=harTrng, method=\"class\")\r\nrfPrediction <- predict(rfModel, harVal, type = \"class\")\r\n#Plot confusion matrix\r\nconfusionMatrix(rfPrediction, harVal$classe)\r\n```\r\n\r\n```\r\n## Confusion Matrix and Statistics\r\n## \r\n##           Reference\r\n## Prediction    A    B    C    D    E\r\n##          A 1115    4    0    0    0\r\n##          B    1  755   10    0    0\r\n##          C    0    0  674   10    0\r\n##          D    0    0    0  633    1\r\n##          E    0    0    0    0  720\r\n## \r\n## Overall Statistics\r\n##                                           \r\n##                Accuracy : 0.9934          \r\n##                  95% CI : (0.9903, 0.9957)\r\n##     No Information Rate : 0.2845          \r\n##     P-Value [Acc > NIR] : < 2.2e-16       \r\n##                                           \r\n##                   Kappa : 0.9916          \r\n##  Mcnemar's Test P-Value : NA              \r\n## \r\n## Statistics by Class:\r\n## \r\n##                      Class: A Class: B Class: C Class: D Class: E\r\n## Sensitivity            0.9991   0.9947   0.9854   0.9844   0.9986\r\n## Specificity            0.9986   0.9965   0.9969   0.9997   1.0000\r\n## Pos Pred Value         0.9964   0.9856   0.9854   0.9984   1.0000\r\n## Neg Pred Value         0.9996   0.9987   0.9969   0.9970   0.9997\r\n## Prevalence             0.2845   0.1935   0.1744   0.1639   0.1838\r\n## Detection Rate         0.2842   0.1925   0.1718   0.1614   0.1835\r\n## Detection Prevalence   0.2852   0.1953   0.1744   0.1616   0.1835\r\n## Balanced Accuracy      0.9988   0.9956   0.9911   0.9921   0.9993\r\n```\r\nWe observe that the Random Forest algorithm gives the highest prediction accuracy amongst the three models. The model accuracy is 99.34% and the out of sample error is estimated at 0.66% which is the lowest amongst the three models we have tried.  \r\n\r\n##Prediction  \r\n\r\nWe choose the random forest model for prediction as follows:-  \r\n\r\n```r\r\nprediction <- predict(rfModel, harTest, type=\"class\")\r\n#Display the 20 predicted values of classe for the test dataset \r\nprediction\r\n```\r\n\r\n```\r\n##  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 \r\n##  B  A  B  A  A  E  D  B  A  A  B  C  B  A  E  E  A  B  B  B \r\n## Levels: A B C D E\r\n```\r\n\r\n```r\r\n#Function to create files for loading data\r\npml_write_files = function(x){\r\n  n = length(x)\r\n  for(i in 1:n){\r\n    filename = paste0(\"problem_id_\",i,\".txt\")\r\n    write.table(x[i],file=filename,quote=FALSE,row.names=FALSE,col.names=FALSE)\r\n  }\r\n}\r\n#Create 20 files with prediction for each test case\r\npml_write_files(prediction)\r\n```\r\n#References  \r\nURL: http://groupware.les.inf.puc-rio.br/har  \r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}